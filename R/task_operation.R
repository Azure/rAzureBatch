#' Add a task to the specified job.
#'
#' @param jobId The id of the job to which the task is to be added.
#' @param taskId A string that uniquely identifies the task within the job.
#' @param ... Further named parameters
#' \itemize{
#'  \item{"resourceFiles"}: {A list of files that the Batch service will download to the compute node before running the command line.}
#'  \item{"args"}: {Arguments in the foreach parameters that will be used for the task running.}
#'  \item{"packages"}: {A list of packages that the Batch service will download to the compute node.}
#'  \item{"envir"}: {The R environment that the task will run under.}
#'}
#' @return The response of task
#' @examples
#' addTask(job-001, task-001)
addTask <- function(jobId, taskId = "default", ...){
  batchCredentials <- getBatchCredentials()
  storageCredentials <- getStorageCredentials()

  args <- list(...)
  .doAzureBatchGlobals <- args$envir
  argsList <- args$args
  packages <- args$packages
  argResourceFiles <- args$resourceFiles

  if(!is.null(argsList)){
    assign('argsList', argsList, .doAzureBatchGlobals)
  }

  envFile <- paste0(taskId, ".rds")
  saveRDS(.doAzureBatchGlobals, file = envFile)
  uploadBlob(jobId, paste0(getwd(), "/", envFile))
  file.remove(envFile)

  sasToken <- constructSas("2016-11-30", "r", "c", jobId, storageCredentials$key)

  taskPrep <- .getInstallationCommand(packages)
  rCommand <- sprintf("Rscript --vanilla --verbose $AZ_BATCH_JOB_PREP_WORKING_DIR/%s %s %s > %s.txt", "worker.R", "$AZ_BATCH_TASK_WORKING_DIR", envFile, taskId)

  resultFile <- paste0(taskId, "-result", ".rds")
  autoUploadCommand <- sprintf("env PATH=$PATH blobxfer %s %s %s --upload --saskey $BLOBXFER_SASKEY --remoteresource result/%s", storageCredentials$name, jobId, resultFile, resultFile)
  stdoutUploadCommand <- sprintf("env PATH=$PATH blobxfer %s %s $AZ_BATCH_TASK_DIR/%s --upload --saskey $BLOBXFER_SASKEY --remoteresource %s", storageCredentials$name, jobId, "stdout.txt", paste0("stdout/", taskId, "-stdout.txt"))
  stderrUploadCommand <- sprintf("env PATH=$PATH blobxfer %s %s $AZ_BATCH_TASK_DIR/%s --upload --saskey $BLOBXFER_SASKEY --remoteresource %s", storageCredentials$name, jobId, "stderr.txt", paste0("stderr/", taskId, "-stderr.txt"))
  logsCommand <- sprintf("env PATH=$PATH blobxfer %s %s %s --upload --saskey $BLOBXFER_SASKEY --remoteresource logs/%s", storageCredentials$name, jobId, paste0(taskId, ".txt"), paste0(taskId, ".txt"))

  commands <- c("export PATH=/anaconda/envs/py35/bin:$PATH", rCommand, autoUploadCommand, stdoutUploadCommand, stderrUploadCommand, logsCommand)
  if(taskPrep != ""){
    commands <- c(taskPrep, commands)
  }

  resourceFiles <- list(generateResourceFile(storageCredentials$name, jobId, envFile, sasToken))

  if(!is.null(argResourceFiles)){
    resourceFiles <- c(resourceFiles, argResourceFiles)
  }

  sasToken <- constructSas("2016-11-30", "rwcl", "c", jobId, storageCredentials$key)
  sasQuery <- generateSasUrl(sasToken)

  setting = list(name = "BLOBXFER_SASKEY",
                 value = sasQuery)

  body = list(id = taskId,
              commandLine = .linuxWrapCommands(commands),
              userIdentity = list(
                autoUser = list(
                  scope = "task",
                  elevationLevel = "admin"
                )
              ),
              resourceFiles = resourceFiles,
              environmentSettings = list(setting))

  size <- nchar(rjson::toJSON(body, method="C"))

  headers <- c()
  headers['Content-Length'] <- size
  headers['Content-Type'] <- "application/json;odata=minimalmetadata"

  request <- AzureRequest$new(
    method = "POST",
    path = paste0("/jobs/", jobId, "/tasks"),
    query = list("api-version" = apiVersion),
    headers = headers
  )

  callBatchService(request, batchCredentials, body)
}

addTaskMerge <- function(jobId, taskId = "default", dependsOn, ...){
  batchCredentials <- getBatchCredentials()
  storageCredentials <- getStorageCredentials()

  args <- list(...)
  .doAzureBatchGlobals <- args$envir
  argsList <- args$args
  packages <- args$packages
  numOfTasks <- args$numOfTasks

  if(!is.null(argsList)){
    assign('argsList', argsList, .doAzureBatchGlobals)
  }

  envFile <- paste0(taskId, ".rds")
  saveRDS(.doAzureBatchGlobals, file = envFile)
  uploadBlob(jobId, paste0(getwd(), "/", envFile))
  file.remove(envFile)

  sasToken <- constructSas("2016-11-30", "r", "c", jobId, storageCredentials$key)

  taskPrep <- .getInstallationCommand(packages)
  rCommand <- sprintf("Rscript --vanilla --verbose $AZ_BATCH_JOB_PREP_WORKING_DIR/%s %s %s %s %s %s > %s.txt", "merger.R", "$AZ_BATCH_TASK_WORKING_DIR", envFile, length(dependsOn), jobId, numOfTasks, taskId)

  resultFile <- paste0(taskId, "-result", ".rds")
  logsCommand <- sprintf("env PATH=$PATH blobxfer %s %s %s --upload --saskey $BLOBXFER_SASKEY --remoteresource logs/%s", storageCredentials$name, jobId, paste0(taskId, ".txt"), paste0(taskId, ".txt"))
  autoUploadCommand <- sprintf("env PATH=$PATH blobxfer %s %s %s --upload --saskey $BLOBXFER_SASKEY --remoteresource result/%s", storageCredentials$name, jobId, resultFile, resultFile)
  downloadCommand <- sprintf("env PATH=$PATH blobxfer %s %s %s --download --saskey $BLOBXFER_SASKEY --remoteresource . --include result/*.rds", storageCredentials$name, jobId, "$AZ_BATCH_TASK_WORKING_DIR")
  logsCommand <- sprintf("env PATH=$PATH blobxfer %s %s %s --upload --saskey $BLOBXFER_SASKEY --remoteresource logs/%s", storageCredentials$name, jobId, paste0(taskId, ".txt"), paste0(taskId, ".txt"))

  commands <- c("export PATH=/anaconda/envs/py35/bin:$PATH", downloadCommand, rCommand, logsCommand, autoUploadCommand)
  if(taskPrep != ""){
    commands <- c(taskPrep, commands)
  }

  resourceFiles <- list(generateResourceFile(storageCredentials$name, jobId, envFile, sasToken))

  sasToken <- constructSas("2016-11-30", "rwcl", "c", jobId, storageCredentials$key)
  sasQuery <- generateSasUrl(sasToken)

  setting = list(name = "BLOBXFER_SASKEY",
                 value = sasQuery)

  body = list(id = taskId,
              commandLine = .linuxWrapCommands(commands),
              userIdentity = list(
                autoUser = list(
                  scope = "task",
                  elevationLevel = "admin"
                )
              ),
              resourceFiles = resourceFiles,
              environmentSettings = list(setting),
              dependsOn = list(taskIds = dependsOn))

  size <- nchar(rjson::toJSON(body, method="C"))

  headers <- c()
  headers['Content-Length'] <- size
  headers['Content-Type'] <- "application/json;odata=minimalmetadata"

  request <- AzureRequest$new(
    method = "POST",
    path = paste0("/jobs/", jobId, "/tasks"),
    query = list("api-version" = apiVersion),
    headers = headers
  )

  callBatchService(request, batchCredentials, body)
}

listTask <- function(jobId, ...){
  batchCredentials <- getBatchCredentials()

  request <- AzureRequest$new(
    method = "GET",
    path = paste0("/jobs/", jobId, "/tasks"),
    query = list("api-version" = apiVersion)
  )

  callBatchService(request, batchCredentials)
}

waitForTasksToComplete <- function(jobId, timeout, ...){
  print("Waiting for tasks to complete. . .")

  args <- list(...)
  progress <- args$progress

  if(is.null(args$tasks)){
    stop("The number of tasks was not initialized.")
  }

  numOfTasks <- args$tasks
  pb <- txtProgressBar(min = 0, max = numOfTasks, style = 3)

  timeToTimeout <- Sys.time() + timeout

  while(Sys.time() < timeToTimeout){
    tasks <- listTask(jobId)

    taskStates <- lapply(tasks$value, function(x) x$state != "completed")
    count <- 0
    for(i in 1:length(taskStates)){
      if(taskStates[[i]] == FALSE){
        count <- count + 1
      }
    }

    setTxtProgressBar(pb, count)

    if(all(taskStates == FALSE)){
      return(0);
    }

    Sys.sleep(10)
  }

  stop("A timeout has occurred when waiting for tasks to complete.")
}
